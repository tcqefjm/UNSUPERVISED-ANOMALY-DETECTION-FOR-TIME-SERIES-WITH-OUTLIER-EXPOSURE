{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, sys, random, io, urllib\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "from IPython.display import Image, display\n",
    "\n",
    "USE_CUDA = (torch.backends.cudnn.version() != None)\n",
    "\n",
    "seed_value = 1234\n",
    "rd.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "ori_dataset = pd.read_csv('./data/fraud_dataset_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ori_dataset.pop('label')\n",
    "\n",
    "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT', 'BUKRS', 'WAERS']\n",
    "\n",
    "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])\n",
    "\n",
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "numeric_attr = ori_dataset[numeric_attr_names] + 1e-4\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "ori_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())\n",
    "\n",
    "ori_subset_transformed = pd.concat([ori_dataset_categ_transformed, ori_dataset_numeric_attr], axis = 1)\n",
    "ori_subset_transformed = ori_subset_transformed[label == 'regular'][:320000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.map_L1 = nn.Linear(input_size, hidden_size[0], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight)\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0)\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L2 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L3 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L4 = nn.Linear(hidden_size[2], hidden_size[3], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_R4 = torch.nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L5 = nn.Linear(hidden_size[3], hidden_size[4], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "        nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "        self.map_R5 = torch.nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_R4(self.map_L4(x))\n",
    "        x = self.map_R5(self.map_L5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.map_L1 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight)\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0)\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L2 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L3 = nn.Linear(hidden_size[2], hidden_size[3], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L4 = nn.Linear(hidden_size[3], hidden_size[4], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L5 = nn.Linear(hidden_size[4], output_size, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "        nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "        self.map_S5 = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_R4(self.map_L4(x))\n",
    "        x = self.map_S5(self.map_L5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "encoder_eval = Encoder(input_size=ori_subset_transformed.shape[1], hidden_size=[256, 64, 16, 4, 2])\n",
    "decoder_eval = Decoder(output_size=ori_subset_transformed.shape[1], hidden_size=[2, 4, 16, 64, 256])\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_eval = encoder_eval.cuda()\n",
    "    decoder_eval = decoder_eval.cuda()\n",
    "\n",
    "encoder_eval.load_state_dict(torch.load('./models/20190818-03_22_18_ep_401_encoder_model.pth'))\n",
    "decoder_eval.load_state_dict(torch.load('./models/20190818-03_22_18_ep_401_decoder_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.map_L1 = nn.Linear(input_size, hidden_size[0], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight)\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0)\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L2 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L3 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L4 = nn.Linear(hidden_size[2], output_size, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_L4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "nz = 16\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.map_L1 = nn.Linear(input_size, hidden_size[0], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight)\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0)\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L2 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L3 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L4 = nn.Linear(hidden_size[2], output_size, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_S4 = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_S4(self.map_L4(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "discriminator_train = Discriminator(input_size=ori_subset_transformed.shape[1], hidden_size=[256, 64, 16], output_size=1)\n",
    "generator_train = Generator(input_size=nz, hidden_size=[64, 128, 256], output_size=ori_subset_transformed.shape[1])\n",
    "\n",
    "if USE_CUDA:\n",
    "    discriminator_train = discriminator_train.cuda()\n",
    "    generator_train = generator_train.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_criterion = torch.nn.HingeEmbeddingLoss()\n",
    "\n",
    "if USE_CUDA:\n",
    "    reconstruction_criterion = reconstruction_criterion.cuda()\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n",
    "discriminator_optimizer = optim.RMSprop(discriminator_train.parameters(), lr=learning_rate)\n",
    "generator_optimizer = optim.RMSprop(generator_train.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "mini_batch_size = 128\n",
    "\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "if USE_CUDA:\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "epoch_discriminator_losses = []\n",
    "epoch_generator_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_value = 0.015\n",
    "n_critic = 5\n",
    "\n",
    "beta = 0.1\n",
    "\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "discriminator_train.train()\n",
    "generator_train.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mini_batch_count = 0\n",
    "\n",
    "    batch_discriminator_losses = 0.0\n",
    "    batch_generator_losses = 0.0\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for mini_batch_data in dataloader:\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        if USE_CUDA:\n",
    "            mini_batch_torch = torch.cuda.FloatTensor(mini_batch_data)\n",
    "        else:\n",
    "            mini_batch_torch = torch.FloatTensor(mini_batch_data)\n",
    "\n",
    "        # =================== discriminator training ===================\n",
    "\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "        noise = torch.randn(mini_batch_size, nz)\n",
    "        if USE_CUDA:\n",
    "            noise = noise.cuda()\n",
    "\n",
    "        g_fake = generator_train(noise)\n",
    "\n",
    "        d_real = discriminator_train(mini_batch_torch)\n",
    "        d_fake = discriminator_train(g_fake.detach())\n",
    "\n",
    "        discriminator_loss = -torch.mean(d_real) + torch.mean(d_fake)\n",
    "\n",
    "        discriminator_loss.backward()\n",
    "        batch_discriminator_losses += discriminator_loss.item()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        for p in discriminator_train.parameters():\n",
    "            p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "        # =================== generator training =======================\n",
    "\n",
    "        if mini_batch_count % n_critic == 0:\n",
    "            generator_train.zero_grad()\n",
    "\n",
    "            rec_input = decoder_eval(encoder_eval(g_fake)) - g_fake\n",
    "            if USE_CUDA:\n",
    "                rec_loss = beta * reconstruction_criterion(rec_input, torch.cuda.FloatTensor([-1]))\n",
    "            else:\n",
    "                rec_loss = beta * reconstruction_criterion(rec_input, torch.FloatTensor([-1]))\n",
    "\n",
    "            d_fake = discriminator_train(g_fake)\n",
    "            generator_loss = -torch.mean(d_fake) + rec_loss\n",
    "\n",
    "            generator_loss.backward()\n",
    "            batch_generator_losses += generator_loss.item()\n",
    "            generator_optimizer.step()\n",
    "\n",
    "    epoch_discriminator_loss = batch_discriminator_losses / mini_batch_count\n",
    "    epoch_discriminator_losses.extend([epoch_discriminator_loss])\n",
    "\n",
    "    epoch_generator_loss = batch_generator_losses / mini_batch_count\n",
    "    epoch_generator_losses.extend([epoch_generator_loss])\n",
    "\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG TRAIN {}] epoch: {:04}/{:04}, discriminator loss: {:.4f}'.format(now, epoch + 1, num_epochs, epoch_discriminator_loss))\n",
    "    print('[LOG TRAIN {}] epoch: {:04}/{:04}, generator loss: {:.4f}'.format(now, epoch + 1, num_epochs, epoch_generator_loss))\n",
    "\n",
    "    model_name = \"{}_ep_{}_wgan_discriminator.pth\".format(now, (epoch+1))\n",
    "    torch.save(discriminator_train.state_dict(), os.path.join(\"./models\", model_name))\n",
    "\n",
    "    model_name = \"{}_ep_{}_wgan_generator.pth\".format(now, (epoch+1))\n",
    "    torch.save(generator_train.state_dict(), os.path.join(\"./models\", model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(epoch_discriminator_losses)), epoch_discriminator_losses)\n",
    "\n",
    "plt.title('Discriminator training performance')\n",
    "\n",
    "plt.xlabel('training epochs')\n",
    "plt.ylabel('discrimination loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(epoch_generator_losses)), epoch_generator_losses)\n",
    "\n",
    "plt.title('Generator training performance')\n",
    "\n",
    "plt.xlabel('training epochs')\n",
    "plt.ylabel('generation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_eval = Generator(input_size=nz, hidden_size=[64, 128, 256], output_size=ori_subset_transformed.shape[1])\n",
    "\n",
    "if USE_CUDA:\n",
    "    generator_eval = generator_eval.cuda()\n",
    "\n",
    "generator_eval.load_state_dict(torch.load('./models/20200419-00_27_05_ep_500_wgan_generator.pth'))\n",
    "\n",
    "generator_eval.eval()\n",
    "\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "noise = torch.randn(128000, nz)\n",
    "if USE_CUDA:\n",
    "    noise = noise.cuda()\n",
    "g_all = generator_eval(noise).cpu().detach()\n",
    "\n",
    "# g_all = torch.from_numpy(pd.read_csv('./data/outlier_009_081.csv')[:128000].values).float()\n",
    "\n",
    "torch_dataset = torch.cat((torch_dataset, g_all), dim=0)\n",
    "dataloader_eval = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "if USE_CUDA:\n",
    "    dataloader_eval = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = 0\n",
    "\n",
    "for enc_transactions_batch in dataloader_eval:\n",
    "    z_enc_transactions_batch = encoder_eval(enc_transactions_batch)\n",
    "\n",
    "    if batch_count == 0:\n",
    "        z_enc_transactions_all = z_enc_transactions_batch\n",
    "    else:\n",
    "        z_enc_transactions_all = torch.cat((z_enc_transactions_all, z_enc_transactions_batch), dim=0)\n",
    "\n",
    "    batch_count += 1\n",
    "\n",
    "z_enc_transactions_all = z_enc_transactions_all.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "regular_data = z_enc_transactions_all[:ori_subset_transformed.shape[0]]\n",
    "outliers = z_enc_transactions_all[ori_subset_transformed.shape[0]:]\n",
    "\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', marker=\"o\", label='regular', edgecolors='w', linewidth=0.5)\n",
    "ax.scatter(outliers[:, 0], outliers[:, 1], c='C1', marker=\"x\", label='outlier', edgecolors='w', s=60)\n",
    "\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 5\n",
    "radius = 0.8\n",
    "\n",
    "x_centroid = (radius * np.sin(np.linspace(0, 2 * np.pi, tau, endpoint=False)) + 1) / 2\n",
    "y_centroid = (radius * np.cos(np.linspace(0, 2 * np.pi, tau, endpoint=False)) + 1) / 2\n",
    "\n",
    "mu_gauss = np.vstack([x_centroid, y_centroid]).T\n",
    "\n",
    "def compute_euclid_distance(x, y):\n",
    "    euclidean_distance = np.sqrt(np.sum((x - y) ** 2, axis=1))\n",
    "\n",
    "    return euclidean_distance\n",
    "\n",
    "distances = np.apply_along_axis(func1d=compute_euclid_distance, axis=1, arr=z_enc_transactions_all, y=mu_gauss)\n",
    "\n",
    "mode_divergence = np.min(distances, axis=1)\n",
    "\n",
    "cluster_ids = np.argmin(distances, axis=1)\n",
    "\n",
    "mode_divergence_all_scaled = np.asarray(mode_divergence)\n",
    "\n",
    "for cluster_id in np.unique(cluster_ids).tolist():\n",
    "    mask = cluster_ids == cluster_id\n",
    "    mode_divergence_all_scaled[mask] = (mode_divergence[mask] - mode_divergence[mask].min()) / (mode_divergence[mask].ptp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [0] * ori_subset_transformed.shape[0] + [1] * (z_enc_transactions_all.shape[0] - ori_subset_transformed.shape[0])\n",
    "plot_data = pd.concat([pd.Series(mode_divergence_all_scaled, name='mode_divergence'),\n",
    "                       pd.Series(label, name='label'),\n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                       axis=1)\n",
    "\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "for mode in range(0, num_clusters):\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    regular_data = z_mode[z_mode['label'] == 0]\n",
    "    outliers = z_mode[z_mode['label'] == 1]\n",
    "\n",
    "    axes[mode].scatter(regular_data.index, regular_data['mode_divergence'],\n",
    "                       c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "\n",
    "    axes[mode].scatter(outliers.index, outliers['mode_divergence'],\n",
    "                       c='C1', marker='x', s=120, linewidth=3, label='outlier', edgecolors='w')\n",
    "\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=24)\n",
    "\n",
    "    axes[mode].set_ylim([0.0, 0.7])\n",
    "\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "axes[0].set_ylabel('mode divergence $MD$', fontsize=20)\n",
    "\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0.,\n",
    "           bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_criterion_categorical_eval = nn.BCEWithLogitsLoss(reduction='none')\n",
    "reconstruction_criterion_numeric_eval = nn.MSELoss(reduction='none')\n",
    "\n",
    "if USE_CUDA:\n",
    "    reconstruction_criterion_categorical_eval = reconstruction_criterion_categorical_eval.cuda()\n",
    "    reconstruction_criterion_numeric_eval = reconstruction_criterion_numeric_eval.cuda()\n",
    "\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "batch_count = 0\n",
    "\n",
    "for enc_transactions_batch in dataloader_eval:\n",
    "    z_enc_transactions_batch = encoder_eval(enc_transactions_batch)\n",
    "\n",
    "    reconstruction_batch = decoder_eval(z_enc_transactions_batch)\n",
    "\n",
    "    input_cat_all = enc_transactions_batch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "    input_num_all = enc_transactions_batch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "    rec_cat_all = reconstruction_batch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "    rec_num_all = reconstruction_batch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "    rec_error_cat_all = reconstruction_criterion_categorical_eval(input=rec_cat_all, target=input_cat_all).mean(dim=1)\n",
    "    rec_error_num_all = reconstruction_criterion_numeric_eval(input=rec_num_all, target=input_num_all).mean(dim=1)\n",
    "\n",
    "    rec_error_all_batch = rec_error_cat_all + rec_error_num_all\n",
    "\n",
    "    if batch_count == 0:\n",
    "        rec_error_all = rec_error_all_batch\n",
    "    else:\n",
    "        rec_error_all = torch.cat((rec_error_all, rec_error_all_batch), dim=0)\n",
    "\n",
    "    batch_count += 1\n",
    "\n",
    "rec_error_all = rec_error_all.cpu().detach().numpy()\n",
    "\n",
    "rec_error_all_scaled = np.asarray(rec_error_all)\n",
    "\n",
    "for cluster_id in np.unique(cluster_ids).tolist():\n",
    "    mask = cluster_ids == cluster_id\n",
    "    rec_error_all_scaled[mask] = (rec_error_all[mask] - rec_error_all[mask].min()) / (rec_error_all[mask].ptp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([pd.Series(rec_error_all_scaled, name='rec_error'),\n",
    "                       pd.Series(label, name='label'),\n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                       axis=1)\n",
    "\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "for mode in range(0, num_clusters):\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    regular_data = z_mode[z_mode['label'] == 0]\n",
    "    outliers = z_mode[z_mode['label'] == 1]\n",
    "\n",
    "    axes[mode].scatter(regular_data.index, regular_data['rec_error'],\n",
    "                       c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "\n",
    "    axes[mode].scatter(outliers.index, outliers['rec_error'],\n",
    "                       c='C1', marker='x', s=120, linewidth=3, label='outlier', edgecolors='w')\n",
    "\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=24)\n",
    "\n",
    "    axes[mode].set_ylim([0.0, 0.8])\n",
    "\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "axes[0].set_ylabel('reconstruction error $RE$', fontsize=20)\n",
    "\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0.,\n",
    "           bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = 0\n",
    "\n",
    "while generated < 267000:\n",
    "    noise = torch.randn(mini_batch_size, nz)\n",
    "    if USE_CUDA:\n",
    "        noise = noise.cuda()\n",
    "    g_batch = generator_eval(noise).detach()\n",
    "\n",
    "    z_enc_transactions_batch = encoder_eval(g_batch).detach()\n",
    "    reconstruction_batch = decoder_eval(z_enc_transactions_batch).detach()\n",
    "    z_enc_transactions_batch = z_enc_transactions_batch.cpu().numpy()\n",
    "\n",
    "    distances_batch = np.apply_along_axis(func1d=compute_euclid_distance, axis=1, arr=z_enc_transactions_batch, y=mu_gauss)\n",
    "    mode_divergence_batch = np.min(distances_batch, axis=1)\n",
    "\n",
    "    input_cat_all = g_batch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "    input_num_all = g_batch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "    rec_cat_all = reconstruction_batch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "    rec_num_all = reconstruction_batch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "    rec_error_cat_all = reconstruction_criterion_categorical_eval(input=rec_cat_all, target=input_cat_all).mean(dim=1)\n",
    "    rec_error_num_all = reconstruction_criterion_numeric_eval(input=rec_num_all, target=input_num_all).mean(dim=1)\n",
    "\n",
    "    rec_error_all_batch = (rec_error_cat_all + rec_error_num_all).cpu().numpy()\n",
    "    g_batch = g_batch.cpu().numpy()[(mode_divergence_batch > 0.09) & (rec_error_all_batch > 0.81)]\n",
    "\n",
    "    pd.DataFrame(g_batch).to_csv('./data/outlier_009_081.csv', header=0, index=0, mode='a')\n",
    "    generated += g_batch.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
