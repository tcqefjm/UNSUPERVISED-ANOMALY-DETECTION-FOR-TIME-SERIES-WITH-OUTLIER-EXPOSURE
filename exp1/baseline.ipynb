{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, sys, random, io, urllib\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = (torch.backends.cudnn.version() != None)\n",
    "\n",
    "seed_value = 1234\n",
    "rd.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "if not os.path.exists('./data'): os.makedirs('./data')\n",
    "if not os.path.exists('./models'): os.makedirs('./models')\n",
    "\n",
    "ori_dataset = pd.read_csv('./data/fraud_dataset_v2.csv')\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] Transactional dataset of {} rows and {} columns loaded'.format(now, ori_dataset.shape[0], ori_dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ori_dataset.pop('label')\n",
    "\n",
    "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT', 'BUKRS', 'WAERS']\n",
    "\n",
    "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])\n",
    "ori_dataset_categ_transformed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "numeric_attr = ori_dataset[numeric_attr_names] + 1e-4\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "ori_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())\n",
    "\n",
    "ori_subset_transformed = pd.concat([ori_dataset_categ_transformed, ori_dataset_numeric_attr], axis = 1)\n",
    "ori_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.map_L1 = nn.Linear(input_size, hidden_size[0], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight)\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0)\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L2 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L3 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L4 = nn.Linear(hidden_size[2], hidden_size[3], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_R4 = torch.nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L5 = nn.Linear(hidden_size[3], hidden_size[4], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "        nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "        self.map_R5 = torch.nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_R4(self.map_L4(x))\n",
    "        x = self.map_R5(self.map_L5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "encoder_train = Encoder(input_size=ori_subset_transformed.shape[1], hidden_size=[256, 64, 16, 4, 2])\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_train = encoder_train.cuda()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder-generator architecture:\\n\\n{}\\n'.format(now, encoder_train))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.map_L1 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight)\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0)\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L2 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L3 = nn.Linear(hidden_size[2], hidden_size[3], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L4 = nn.Linear(hidden_size[3], hidden_size[4], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L5 = nn.Linear(hidden_size[4], output_size, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "        nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "        self.map_S5 = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_R4(self.map_L4(x))\n",
    "        x = self.map_S5(self.map_L5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "decoder_train = Decoder(output_size=ori_subset_transformed.shape[1], hidden_size=[2, 4, 16, 64, 256])\n",
    "\n",
    "if USE_CUDA:\n",
    "    decoder_train = decoder_train.cuda()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.map_L1 = nn.Linear(input_size, hidden_size[0], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight)\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0)\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L2 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L3 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.map_L4 = nn.Linear(hidden_size[2], output_size, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_S4 = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_S4(self.map_L4(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "discriminator_train = Discriminator(input_size=2, hidden_size=[256, 16, 4], output_size=1)\n",
    "\n",
    "if USE_CUDA:\n",
    "    discriminator_train = discriminator_train.cuda()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] discriminator architecture:\\n\\n{}\\n'.format(now, discriminator_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_criterion_categorical = nn.BCELoss(reduction='mean')\n",
    "reconstruction_criterion_numeric = nn.MSELoss(reduction='mean')\n",
    "\n",
    "if USE_CUDA:\n",
    "    reconstruction_criterion_categorical = reconstruction_criterion_categorical.cuda()\n",
    "    reconstruction_criterion_numeric = reconstruction_criterion_numeric.cuda()\n",
    "\n",
    "learning_rate_enc = 1e-3\n",
    "learning_rate_dec = 1e-3\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder_train.parameters(), lr=learning_rate_enc)\n",
    "decoder_optimizer = optim.Adam(decoder_train.parameters(), lr=learning_rate_dec)\n",
    "\n",
    "discriminator_criterion = nn.BCELoss()\n",
    "\n",
    "if USE_CUDA:\n",
    "    discriminator_criterion = discriminator_criterion.cuda()\n",
    "\n",
    "learning_rate_dis_z = 1e-5\n",
    "\n",
    "discriminator_optimizer = optim.Adam(discriminator_train.parameters(), lr=learning_rate_dis_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 5\n",
    "radius = 0.8\n",
    "sigma = 0.01\n",
    "dim = 2\n",
    "\n",
    "x_centroid = (radius * np.sin(np.linspace(0, 2 * np.pi, tau, endpoint=False)) + 1) / 2\n",
    "y_centroid = (radius * np.cos(np.linspace(0, 2 * np.pi, tau, endpoint=False)) + 1) / 2\n",
    "\n",
    "mu_gauss = np.vstack([x_centroid, y_centroid]).T\n",
    "\n",
    "samples_per_gaussian = 100000\n",
    "\n",
    "for i, mu in enumerate(mu_gauss):\n",
    "    if i == 0:\n",
    "        z_continous_samples_all = np.random.normal(mu, sigma, size=(samples_per_gaussian, dim))\n",
    "    else:\n",
    "        z_continous_samples = np.random.normal(mu, sigma, size=(samples_per_gaussian, dim))\n",
    "        z_continous_samples_all = np.vstack([z_continous_samples_all, z_continous_samples])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(z_continous_samples_all[:, 0], z_continous_samples_all[:, 1], c='C0', marker=\"o\", edgecolors='w', linewidth=0.5)\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')\n",
    "ax.set_title('Prior Latent Space Distribution $p(z)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "mini_batch_size = 128\n",
    "\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "if USE_CUDA:\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "epoch_reconstruction_losses = []\n",
    "epoch_discriminator_losses = []\n",
    "epoch_generator_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    mini_batch_count = 0\n",
    "\n",
    "    batch_reconstruction_losses = 0.0\n",
    "    batch_discriminator_losses = 0.0\n",
    "    batch_generator_losses = 0.0\n",
    "\n",
    "    encoder_train.train()\n",
    "    decoder_train.train()\n",
    "    discriminator_train.train()\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for mini_batch_data in dataloader:\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        if USE_CUDA:\n",
    "            mini_batch_torch = torch.cuda.FloatTensor(mini_batch_data)\n",
    "        else:\n",
    "            mini_batch_torch = torch.FloatTensor(mini_batch_data)\n",
    "\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "        # =================== reconstruction phase =====================\n",
    "\n",
    "        z_sample = encoder_train(mini_batch_torch)\n",
    "        mini_batch_reconstruction = decoder_train(z_sample)\n",
    "\n",
    "        batch_cat = mini_batch_torch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "        batch_num = mini_batch_torch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "        rec_batch_cat = mini_batch_reconstruction[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "        rec_batch_num = mini_batch_reconstruction[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "        rec_error_cat = reconstruction_criterion_categorical(input=rec_batch_cat, target=batch_cat)\n",
    "        rec_error_num = reconstruction_criterion_numeric(input=rec_batch_num, target=batch_num)\n",
    "\n",
    "        reconstruction_loss = rec_error_cat + rec_error_num\n",
    "\n",
    "        reconstruction_loss.backward()\n",
    "\n",
    "        batch_reconstruction_losses += reconstruction_loss.item()\n",
    "\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== regularization phase =====================\n",
    "        # =================== discriminator training ===================\n",
    "\n",
    "        discriminator_train.eval()\n",
    "\n",
    "        z_target_batch = z_continous_samples_all[random.sample(range(0, z_continous_samples_all.shape[0]), mini_batch_size),:]\n",
    "\n",
    "        z_target_batch = torch.FloatTensor(z_target_batch)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            z_target_batch = z_target_batch.cuda()\n",
    "\n",
    "        z_fake_gauss = encoder_train(mini_batch_torch)\n",
    "\n",
    "        d_real_gauss = discriminator_train(z_target_batch)\n",
    "        d_fake_gauss = discriminator_train(z_fake_gauss)\n",
    "\n",
    "        d_real_gauss_target = torch.FloatTensor(torch.ones(d_real_gauss.shape))\n",
    "        d_fake_gauss_target = torch.FloatTensor(torch.zeros(d_fake_gauss.shape))\n",
    "\n",
    "        if USE_CUDA:\n",
    "            d_real_gauss_target = d_real_gauss_target.cuda()\n",
    "            d_fake_gauss_target = d_fake_gauss_target.cuda()\n",
    "\n",
    "        discriminator_loss_real = discriminator_criterion(target=d_real_gauss_target, input=d_real_gauss)\n",
    "        discriminator_loss_fake = discriminator_criterion(target=d_fake_gauss_target, input=d_fake_gauss)\n",
    "\n",
    "        discriminator_loss = discriminator_loss_fake + discriminator_loss_real\n",
    "\n",
    "        discriminator_loss.backward()\n",
    "\n",
    "        batch_discriminator_losses += discriminator_loss.item()\n",
    "\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # =================== regularization phase =====================\n",
    "        # =================== generator training =======================\n",
    "\n",
    "        encoder_train.train()\n",
    "        encoder_train.zero_grad()\n",
    "\n",
    "        z_fake_gauss = encoder_train(mini_batch_torch)\n",
    "\n",
    "        d_fake_gauss = discriminator_train(z_fake_gauss)\n",
    "\n",
    "        d_fake_gauss_target = torch.FloatTensor(torch.ones(d_fake_gauss.shape))\n",
    "\n",
    "        if USE_CUDA:\n",
    "            d_fake_gauss_target = d_fake_gauss_target.cuda()\n",
    "\n",
    "        generator_loss = discriminator_criterion(target=d_fake_gauss_target, input=d_fake_gauss)\n",
    "\n",
    "        batch_generator_losses += generator_loss.item()\n",
    "\n",
    "        generator_loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "    epoch_reconstruction_loss = batch_reconstruction_losses / mini_batch_count\n",
    "    epoch_reconstruction_losses.extend([epoch_reconstruction_loss])\n",
    "\n",
    "    epoch_discriminator_loss = batch_discriminator_losses / mini_batch_count\n",
    "    epoch_discriminator_losses.extend([epoch_discriminator_loss])\n",
    "\n",
    "    epoch_generator_loss = batch_generator_losses / mini_batch_count\n",
    "    epoch_generator_losses.extend([epoch_generator_loss])\n",
    "\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG TRAIN {}] epoch: {:04}/{:04}, reconstruction loss: {:.4f}'.format(now, epoch + 1, num_epochs, epoch_reconstruction_loss))\n",
    "    print('[LOG TRAIN {}] epoch: {:04}/{:04}, discriminator loss: {:.4f}'.format(now, epoch + 1, num_epochs, epoch_discriminator_loss))\n",
    "    print('[LOG TRAIN {}] epoch: {:04}/{:04}, generator loss: {:.4f}'.format(now, epoch + 1, num_epochs, epoch_generator_loss))\n",
    "\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "    encoder_model_name = \"{}_ep_{}_encoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(\"./models\", encoder_model_name))\n",
    "\n",
    "    decoder_model_name = \"{}_ep_{}_decoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(\"./models\", decoder_model_name))\n",
    "\n",
    "    decoder_model_name = \"{}_ep_{}_discriminator_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(discriminator_train.state_dict(), os.path.join(\"./models\", decoder_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(epoch_reconstruction_losses)+1), epoch_reconstruction_losses)\n",
    "\n",
    "plt.title('AAE training performance')\n",
    "\n",
    "plt.xlabel('training epochs')\n",
    "plt.ylabel('reconstruction loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(epoch_discriminator_losses)), epoch_discriminator_losses)\n",
    "\n",
    "plt.title('AENN training performance')\n",
    "\n",
    "plt.xlabel('training epochs')\n",
    "plt.ylabel('discrimination loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(epoch_generator_losses)), epoch_generator_losses)\n",
    "\n",
    "plt.title('AENN training performance')\n",
    "\n",
    "plt.xlabel('training epochs')\n",
    "plt.ylabel('generation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_eval = Encoder(input_size=ori_subset_transformed.shape[1], hidden_size=[256, 64, 16, 4, 2])\n",
    "decoder_eval = Decoder(output_size=ori_subset_transformed.shape[1], hidden_size=[2, 4, 16, 64, 256])\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_eval = encoder_eval.cuda()\n",
    "    decoder_eval = decoder_eval.cuda()\n",
    "\n",
    "encoder_eval.load_state_dict(torch.load('./models/20190818-03_22_18_ep_401_encoder_model.pth'))\n",
    "decoder_eval.load_state_dict(torch.load('./models/20190818-03_22_18_ep_401_decoder_model.pth'))\n",
    "\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "dataloader_eval = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "if USE_CUDA:\n",
    "    dataloader_eval = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = 0\n",
    "\n",
    "for enc_transactions_batch in dataloader_eval:\n",
    "    z_enc_transactions_batch = encoder_eval(enc_transactions_batch)\n",
    "\n",
    "    if batch_count == 0:\n",
    "        z_enc_transactions_all = z_enc_transactions_batch\n",
    "    else:\n",
    "        z_enc_transactions_all = torch.cat((z_enc_transactions_all, z_enc_transactions_batch), dim=0)\n",
    "\n",
    "    batch_count += 1\n",
    "\n",
    "z_enc_transactions_all = z_enc_transactions_all.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "regular_data = z_enc_transactions_all[label == 'regular']\n",
    "global_outliers = z_enc_transactions_all[label == 'global']\n",
    "local_outliers = z_enc_transactions_all[label == 'local']\n",
    "\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', marker=\"o\", label='regular', edgecolors='w', linewidth=0.5)\n",
    "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"x\", label='global', edgecolors='w', s=60)\n",
    "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C3', marker=\"x\", label='local', edgecolors='w', s=60)\n",
    "\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclid_distance(x, y):\n",
    "    euclidean_distance = np.sqrt(np.sum((x - y) ** 2, axis=1))\n",
    "\n",
    "    return euclidean_distance\n",
    "\n",
    "distances = np.apply_along_axis(func1d=compute_euclid_distance, axis=1, arr=z_enc_transactions_all, y=mu_gauss)\n",
    "\n",
    "mode_divergence = np.min(distances, axis=1)\n",
    "\n",
    "cluster_ids = np.argmin(distances, axis=1)\n",
    "\n",
    "mode_divergence_all_scaled = np.asarray(mode_divergence)\n",
    "\n",
    "for cluster_id in np.unique(cluster_ids).tolist():\n",
    "    mask = cluster_ids == cluster_id\n",
    "    mode_divergence_all_scaled[mask] = (mode_divergence[mask] - mode_divergence[mask].min()) / (mode_divergence[mask].ptp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([pd.Series(mode_divergence_all_scaled, name='mode_divergence'),\n",
    "                       pd.Series(label, name='label'),\n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                       axis=1)\n",
    "\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "for mode in range(0, num_clusters):\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    regular_data = z_mode[z_mode['label'] == 'regular']\n",
    "    global_outliers = z_mode[z_mode['label'] == 'global']\n",
    "    local_outliers = z_mode[z_mode['label'] == 'local']\n",
    "\n",
    "    axes[mode].scatter(regular_data.index, regular_data['mode_divergence'],\n",
    "                       c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "\n",
    "    axes[mode].scatter(global_outliers.index, global_outliers['mode_divergence'],\n",
    "                       c='C1', marker='x', s=120, linewidth=3, label='global', edgecolors='w')\n",
    "\n",
    "    axes[mode].scatter(local_outliers.index, local_outliers['mode_divergence'],\n",
    "                       c='C3', marker='x', s=120, linewidth=3, label='local', edgecolors='w')\n",
    "\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=24)\n",
    "\n",
    "    axes[mode].set_ylim([0.0, 1.1])\n",
    "\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "axes[0].set_ylabel('mode divergence $MD$', fontsize=20)\n",
    "\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0.,\n",
    "           bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_criterion_categorical_eval = nn.BCEWithLogitsLoss(reduction='none')\n",
    "reconstruction_criterion_numeric_eval = nn.MSELoss(reduction='none')\n",
    "\n",
    "if USE_CUDA:\n",
    "    reconstruction_criterion_categorical_eval = reconstruction_criterion_categorical_eval.cuda()\n",
    "    reconstruction_criterion_numeric_eval = reconstruction_criterion_numeric_eval.cuda()\n",
    "\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "batch_count = 0\n",
    "\n",
    "for enc_transactions_batch in dataloader_eval:\n",
    "    z_enc_transactions_batch = encoder_eval(enc_transactions_batch)\n",
    "\n",
    "    reconstruction_batch = decoder_eval(z_enc_transactions_batch)\n",
    "\n",
    "    input_cat_all = enc_transactions_batch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "    input_num_all = enc_transactions_batch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "    rec_cat_all = reconstruction_batch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "    rec_num_all = reconstruction_batch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "    rec_error_cat_all = reconstruction_criterion_categorical_eval(input=rec_cat_all, target=input_cat_all).mean(dim=1)\n",
    "    rec_error_num_all = reconstruction_criterion_numeric_eval(input=rec_num_all, target=input_num_all).mean(dim=1)\n",
    "\n",
    "    rec_error_all_batch = rec_error_cat_all + rec_error_num_all\n",
    "\n",
    "    if batch_count == 0:\n",
    "        rec_error_all = rec_error_all_batch\n",
    "    else:\n",
    "        rec_error_all = torch.cat((rec_error_all, rec_error_all_batch), dim=0)\n",
    "\n",
    "    batch_count += 1\n",
    "\n",
    "rec_error_all = rec_error_all.cpu().detach().numpy()\n",
    "\n",
    "rec_error_all_scaled = np.asarray(rec_error_all)\n",
    "\n",
    "for cluster_id in np.unique(cluster_ids).tolist():\n",
    "    mask = cluster_ids == cluster_id\n",
    "    rec_error_all_scaled[mask] = (rec_error_all[mask] - rec_error_all[mask].min()) / (rec_error_all[mask].ptp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([pd.Series(rec_error_all_scaled, name='rec_error'),\n",
    "                       pd.Series(label, name='label'),\n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                       axis=1)\n",
    "\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "for mode in range(0, num_clusters):\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    regular_data = z_mode[z_mode['label'] == 'regular']\n",
    "    global_outliers = z_mode[z_mode['label'] == 'global']\n",
    "    local_outliers = z_mode[z_mode['label'] == 'local']\n",
    "\n",
    "    axes[mode].scatter(regular_data.index, regular_data['rec_error'],\n",
    "                       c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "\n",
    "    axes[mode].scatter(global_outliers.index, global_outliers['rec_error'],\n",
    "                       c='C1', marker='x', s=120, linewidth=3, label='global', edgecolors='w')\n",
    "\n",
    "    axes[mode].scatter(local_outliers.index, local_outliers['rec_error'],\n",
    "                       c='C3', marker='x', s=120, linewidth=3, label='local', edgecolors='w')\n",
    "\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=24)\n",
    "\n",
    "    axes[mode].set_ylim([0.0, 1.1])\n",
    "\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "axes[0].set_ylabel('reconstruction error $RE$', fontsize=20)\n",
    "\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0.,\n",
    "           bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.4\n",
    "\n",
    "anomaly_score = alpha * rec_error_all_scaled + (1.0 - alpha) * mode_divergence_all_scaled\n",
    "\n",
    "plot_data = pd.concat([pd.Series(anomaly_score, name='anomaly_score'),\n",
    "                       pd.Series(label, name='label'),\n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                       axis=1)\n",
    "\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "for mode in range(0, num_clusters):\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    regular_data = z_mode[z_mode['label'] == 'regular']\n",
    "    global_outliers = z_mode[z_mode['label'] == 'global']\n",
    "    local_outliers = z_mode[z_mode['label'] == 'local']\n",
    "\n",
    "    axes[mode].scatter(regular_data.index, regular_data['anomaly_score'],\n",
    "                       c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "\n",
    "    axes[mode].scatter(global_outliers.index, global_outliers['anomaly_score'],\n",
    "                       c='C1', marker='x', s=120, linewidth=3, label='global', edgecolors='w')\n",
    "\n",
    "    axes[mode].scatter(local_outliers.index, local_outliers['anomaly_score'],\n",
    "                       c='C3', marker='x', s=120, linewidth=3, label='local', edgecolors='w')\n",
    "\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=24)\n",
    "\n",
    "    axes[mode].set_ylim([0.0, 1.1])\n",
    "\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "axes[0].set_ylabel('anomaly score $AS$', fontsize=20)\n",
    "\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0.,\n",
    "           bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_dataset['label'] = label\n",
    "ori_dataset['tau'] = cluster_ids\n",
    "ori_dataset['a'] = anomaly_score\n",
    "\n",
    "precision, recall = [], []\n",
    "ths = []\n",
    "da = 0\n",
    "for i in range(num_clusters):\n",
    "    maxregular = ori_dataset[(ori_dataset['label']=='regular') & (ori_dataset['tau']==i)]['a'].max()\n",
    "    t1 = ori_dataset[(ori_dataset['label']!='regular') & (ori_dataset['tau']==i) & (ori_dataset['a']<=maxregular)]['a'].sort_values().reset_index(drop=True)\n",
    "    t2 = ori_dataset[(ori_dataset['label']!='regular') & (ori_dataset['tau']==i) & (ori_dataset['a']>maxregular)]['a']\n",
    "    ths.append(t1 if t1.shape[0] else [t2.min()] if t2.shape[0] else [1])\n",
    "    da += t2.shape[0]\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "for th in product(*ths):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    for i in range(num_clusters):\n",
    "        p = ori_dataset[(anomaly_score>=th[i]) & (cluster_ids==i)]\n",
    "        n = ori_dataset[(anomaly_score<th[i]) & (cluster_ids==i)]\n",
    "        tp += p[p['label']!='regular'].shape[0]\n",
    "        fp += p[p['label']=='regular'].shape[0]\n",
    "        tn += n[n['label']=='regular'].shape[0]\n",
    "        fn += n[n['label']!='regular'].shape[0]\n",
    "    precision.append(tp/(tp+fp) if tp else 1)\n",
    "    recall.append(tp/(tp+fn))\n",
    "\n",
    "total = ori_dataset[ori_dataset['label']!='regular'].shape[0]\n",
    "for i in range(da + 1):\n",
    "    precision.append(1)\n",
    "    recall.append(i/total)\n",
    "\n",
    "mf1 = 0\n",
    "for p, r in zip(precision, recall):\n",
    "    f1 = 2 * p * r / (p + r)\n",
    "    if f1 > mf1:\n",
    "        mf1 = f1\n",
    "        mp, mr = p, r\n",
    "print('precision: {:.4f}, recall: {:.4f}, f1 best: {:.4f}'.format(mp, mr, mf1))\n",
    "\n",
    "data = pd.DataFrame({'precision':precision,'recall':recall})\n",
    "sns.lineplot(x=\"recall\", y=\"precision\", data=data)\n",
    "plt.xlim(0, 1.0)\n",
    "plt.ylim(0, 1.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
